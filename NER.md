<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [NER](#ner)
  - [少样本NER](#少样本ner)
  - [地址解析](#地址解析)
  - [Datasets](#datasets)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


# NER
- [BiLSTM上的CRF，用命名实体识别任务来解释CRF](https://mp.weixin.qq.com/s/2Eq1tSt0Wqxh8MULR27qYA)
- [融合知识的中文医疗实体识别模型](http://www.scicat.cn/yy/20211208/108868.html)
  - 通过构建好的实体库来对文本进行预打标，将预标注结果作为bert模型的输入
- [中文NER | 江南大学提出NFLAT：FLAT的一种改进方案](https://mp.weixin.qq.com/s/-bpr3ySRaPZqRdJgI21A6w)
- [统一NER模型SOTA—W2NER学习笔记](https://mp.weixin.qq.com/s/9A5HXuvVYjHjYb8cn1CYpg)
- [妙啊！MarkBERT](https://mp.weixin.qq.com/s/GDnpvesnX79OS5mhpkd9LA)
- [PERT：一种基于乱序语言模型的预训练模型](https://mp.weixin.qq.com/s/gx6N5QBZozxdZqSOjMKOKA)
- [文本生成序列之前缀语言模型](https://mp.weixin.qq.com/s/WGRRVKiPGR8lZsOkM5Z4Tw)
- [文本生成系列之transformer结构扩展（二）](https://mp.weixin.qq.com/s/brifykEle1Rd7v5F0YxdSg)
- [知识融入语言模型——ERNIE与ERNIE](https://mp.weixin.qq.com/s/trAwVkbwKqUmC5sUbC_S0w)
- [不拆分单词也可以做NLP，哈工大最新模型在多项任务中打败BERT，还能直接训练中文](https://mp.weixin.qq.com/s/UBoMRmymwnw9Ds3S3OW6Mw)
- 


## 少样本NER
- [ACL2022 | 序列标注的小样本NER：融合标签语义的双塔BERT模型](https://mp.weixin.qq.com/s/56OH4d7WDYjuLxWh4kW-1w)
- [微软、UIUC韩家炜组联合出品：少样本NER最新综述](https://mp.weixin.qq.com/s/tiMoFMVdQketm11rdXjiSQ)
- [ACL 2021 | 复旦大学邱锡鹏组：面向不同NER子任务的统一生成框架](https://mp.weixin.qq.com/s/2AePxoar9j4MLQLxMzSf_g)
- [低资源和跨语言NER任务的新进展：词级别数据增强技术](https://mp.weixin.qq.com/s/9vYd9O7BRd_k_56AF5xT0g)
- [缺少训练样本怎么做实体识别？小样本下的NER解决方法汇总](https://mp.weixin.qq.com/s/lRNABGEqf5qxreiR27BpAQ)
- [COLING 2022 | 少样本NER：分散分布原型增强的实体级原型网络](https://mp.weixin.qq.com/s/vdNKuZRg2Umst0TSn3p2Qw)
- [ACL 2022 | 基于自描述网络的小样本命名实体识别](https://mp.weixin.qq.com/s/WUjK6qM7qkLs66aMoLYaIA)
- 


## 地址解析

- [天池中文NLP地址要素解析大赛Top方案总结](https://mp.weixin.qq.com/s/bjbcT0Yt-Q-4KjQSg-3mFQ)
- [BERT+Biaffine结构中文NLP地址要素解析](https://mp.weixin.qq.com/s/o5BZ8-l-rjyJmF0V_G1cNg)

## Datasets
- [中文命名实体识别数据集](https://mp.weixin.qq.com/s/bIRhscHb1VjMAM1axLcUhw)