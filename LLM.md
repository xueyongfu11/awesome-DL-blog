
## LLM

- [大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼](https://zhuanlan.zhihu.com/p/624012908)
- [在一张 24 GB 的消费级显卡上用 RLHF 微调 20B LLMs](https://mp.weixin.qq.com/s/jWBB2BQWHUqJk0OMo3ApXQ)
- [韩国庆熙大学等最新《生成式人工智能AIGC》综述](https://mp.weixin.qq.com/s/w185vd78lIKgpbXzNXiLOg)
- [GPT-4之高考评测](https://mp.weixin.qq.com/s/3NE1DAcIK42rbMRtPUMo8A)
- [ChatGPT 核心技术大起底——InstructGPT：研究人类反馈数据比加大模型规模更重要！](https://mp.weixin.qq.com/s/zzicRhuAbZ8zCOw8ok_S9w)
- [GPT解数学题准确率升至92.5%！微软提出MathPrompter，无需微调即可打造「理科」语言模型](https://mp.weixin.qq.com/s/BR7XDIjb0s07w9OInHrJLg)
- [总结开源可用的Instruct/Prompt Tuning数据](https://zhuanlan.zhihu.com/p/615277009)
- [总结当下可用的大模型LLMs](https://zhuanlan.zhihu.com/p/611403556)
- [RLHF魔法的衍生研究方向](https://mp.weixin.qq.com/s/535LhCV9GJPJGS3Jb5zbew)
- [ChatGPT应用端的Prompt解析：从概念、基本构成、常见任务、构造策略到开源工具与数据集](https://mp.weixin.qq.com/s/QJhqN6tn1FffDKdzWHuCAw)
- [ChatGPT等GPT-3.5系列大模型的鲁棒性如何？](https://mp.weixin.qq.com/s/LGf3Y_k8IokG1rqiX_4YpQ)
- [斯坦福等学者对ChatGPT做了在NLP几乎所有任务上的优劣势分析](https://mp.weixin.qq.com/s/xH89ENEMW6fWRoApLvgRZg)
- [ChatGPT 背后的“功臣”——RLHF 技术详解](https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ)


## 大模型训练

- [BLOOM 训练背后的技术](https://mp.weixin.qq.com/s/-q9opkoAomd9LZL9phm8bA)


## stable-diffusion

- [综述：扩散模型在文本生成中的应用](https://mp.weixin.qq.com/s/4oLlpHanhZ07RmStTlwzXg)